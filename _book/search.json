[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Economic Columns Analysis",
    "section": "",
    "text": "1 문재인 정부 기간 경제 신문의 칼럼 담론 분석 보고서"
  },
  {
    "objectID": "index.html#들어가기-앞서",
    "href": "index.html#들어가기-앞서",
    "title": "Economic Columns Analysis",
    "section": "1.1 들어가기 앞서",
    "text": "1.1 들어가기 앞서\n본 문서는 지난 문재인 대통령 재임 기간(2017년 5월 10일~2022년 5월 9일) 간의 경제 신문사 기고 칼럼을 분석한 보고서이다."
  },
  {
    "objectID": "index.html#데이터-수집",
    "href": "index.html#데이터-수집",
    "title": "Economic Columns Analysis",
    "section": "1.2 데이터 수집",
    "text": "1.2 데이터 수집\n먼저 분석을 위해 경제 신문의 범위를 지정했다. 현재 빅카인즈에 데이터를 공개하는 언론 중 경제 카테고리에 포함되는 언론사는 5곳(한국경제, 매일경제, 서울경제, 파이낸셜뉴스, 헤럴드경제)로, 해당 언론사를 경제신문으로 정했다.\n기사는 빅카인즈(BigKinds)를 통해 수집하였다. 포털 사이트의 기사를 크롤링을 하는 방법도 있으나, 포털 뉴스 특성상 기사가 중복된 내용이거나 지나친 편향성을 지닌 기사가 많아, 본 분석에선 빅카인즈에서 데이터를 별도로 수집하는 방식을 선택했다.\n기사는 문재인 대통령 취임일인 2017년 5월 10일부터 다음 대통령인 윤석열 대통령의 취임일 전날인 2022년 5월 9일까지의 기사를 수집했다.\n\n분석 방법 및 결과는 앞으로의 문서에서 다룬다."
  },
  {
    "objectID": "economic_columns_clustering.html#개요",
    "href": "economic_columns_clustering.html#개요",
    "title": "2  Clustering analysis을 중심으로",
    "section": "2.1 개요",
    "text": "2.1 개요\n본 내용은 칼럼 데이터에 문서 군집 분석 진행한 보고서이다.\n분석 언어는 Python을 사용했으며, 가장 보편적인 군집 분석 방법인 K-Means Clusteringdmf 사용했다. 알고리즘은 Scikit-Learn의 KMeans() 함수를 활용하여 분석을 진행했다.\n군집 분석에 앞서 EDA를 먼저 진행하였다."
  },
  {
    "objectID": "economic_columns_clustering.html#eda",
    "href": "economic_columns_clustering.html#eda",
    "title": "2  Clustering analysis을 중심으로",
    "section": "2.2 EDA",
    "text": "2.2 EDA\n먼저 언론사 별 칼럼 보도 빈도를 확인하였다. 빈도 분석은 필자가 직접 제작한 라이브러리 ’BigKindsParser’의 press_counter 함수를 활용했다.\n\ndf_key = bkp.press_counter(df)\n\nsns.barplot(data = df_key, x = '기사', y = '언론사')\n\nplt.figure(facecolor = 'white')\nplt.show()\n\n\n\n\n&lt;Figure size 960x960 with 0 Axes&gt;"
  },
  {
    "objectID": "economic_columns_clustering.html#키워드-빈도",
    "href": "economic_columns_clustering.html#키워드-빈도",
    "title": "2  Clustering analysis을 중심으로",
    "section": "2.3 키워드 빈도",
    "text": "2.3 키워드 빈도\n언론사 빈도 분석에 이어 단어 빈도 분석을 진행하였다. 시각화는 워드클라우드로 진행하였으며, 분석 알고리즘은 BigKindsParser를 통해 진행했다.\n\nkeywords = bkp.keywords_list(df)\nnews_key = bkp.keyword_parser(keywords)\nnews_key = bkp.duplication_remover(news_key)\nkey = bkp.word_counter(news_key)\nnews_key = bkp.counter_to_DataFrame(key)\n\nwc = WordCloud(font_path='malgun',\n    width = 500,\n    height = 500,\n    background_color='white').generate_from_frequencies(news_key.set_index('단어').to_dict()[\"빈도\"])\n\n\nplt.figure(figsize = (10, 10))\nplt.imshow(wc)\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "economic_columns_clustering.html#언론사-별-키워드-분석",
    "href": "economic_columns_clustering.html#언론사-별-키워드-분석",
    "title": "2  Clustering analysis을 중심으로",
    "section": "2.4 언론사 별 키워드 분석",
    "text": "2.4 언론사 별 키워드 분석\n더 자세한 분석을 위해 언론사 별로 나눈 키워드 분석도 진행했다.\n\n2.4.1 언론사별 키워드 분석 –&gt; 한경\n\nbkp.keywords_wordcloud(df, '한국경제')\n\n\n\n\n\n\n2.4.2 언론사별 키워드 분석 –&gt; 매경\n\nbkp.keywords_wordcloud(df, '매일경제')\n\n\n\n\n\n\n2.4.3 언론사별 키워드 분석 –&gt; 서경\n\nbkp.keywords_wordcloud(df, '서울경제')\n\n\n\n\n\n\n2.4.4 언론사별 키워드 분석 –&gt; 파이낸셜\n\nbkp.keywords_wordcloud(df, '파이낸셜뉴스')\n\n\n\n\n\n\n2.4.5 언론사별 키워드 분석 –&gt; 헤럴드\n\nbkp.keywords_wordcloud(df, '헤럴드경제')"
  },
  {
    "objectID": "economic_columns_clustering.html#tf-idf-분석",
    "href": "economic_columns_clustering.html#tf-idf-분석",
    "title": "2  Clustering analysis을 중심으로",
    "section": "2.5 TF-IDF 분석",
    "text": "2.5 TF-IDF 분석\n언론사별 용어 빈도는 비슷하게 나온다는 점에서, 주류 단어가 아닌 언론사마다 특징적으로 사용한 단어를 추출하기 위해 상대 빈도 분석을 진행하였다.\n상대 빈도 분석 방법은 tf-idf으로 진행하였다.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer \n\n\n2.5.1 언론사별 상대 빈도 분석 –&gt; 한경\n\nhan_words = df[df['언론사'] == '한국경제']\nhan_words = han_words['키워드']\n\ntfidf = TfidfVectorizer()\ntdm = tfidf.fit_transform(han_words)\n\nword_count1 = pd.DataFrame({\n    '단어': tfidf.get_feature_names_out(),\n    '빈도': tdm.sum(axis=0).flat\n}).sort_values('빈도', ascending = False).reset_index(drop = True)\n\nwc = WordCloud(font_path = 'malgun',\n    width = 500,\n    height = 500,\n    background_color='white').generate_from_frequencies(word_count1.set_index('단어').to_dict()[\"빈도\"])\n\n\nplt.figure(figsize = (10, 10))\nplt.imshow(wc)\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n2.5.2 언론사별 상대 빈도 분석 –&gt; 매경\n\nmail_words = df[df['언론사'] == '매일경제']\nmail_words = mail_words['키워드']\n\ntfidf = TfidfVectorizer()\ntdm = tfidf.fit_transform(mail_words)\n\nword_count2 = pd.DataFrame({\n    '단어': tfidf.get_feature_names_out(),\n    '빈도': tdm.sum(axis=0).flat\n}).sort_values('빈도', ascending = False).reset_index(drop = True)\n\nwc = WordCloud(font_path = 'malgun',\n    width = 500,\n    height = 500,\n    background_color='white').generate_from_frequencies(word_count2.set_index('단어').to_dict()[\"빈도\"])\n\nplt.figure(figsize = (10, 10))\nplt.imshow(wc)\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n2.5.3 언론사별 상대 빈도 분석 –&gt; 서경\n\nseo_words = df[df['언론사'] == '서울경제']\nseo_words = seo_words['키워드']\n\ntfidf = TfidfVectorizer()\ntdm = tfidf.fit_transform(seo_words)\n\nword_count3 = pd.DataFrame({\n    '단어': tfidf.get_feature_names_out(),\n    '빈도': tdm.sum(axis=0).flat\n}).sort_values('빈도', ascending = False).reset_index(drop = True)\n\nwc = WordCloud(font_path = 'malgun',\n    width = 500,\n    height = 500,\n    background_color='white').generate_from_frequencies(word_count3.set_index('단어').to_dict()[\"빈도\"])\n\nplt.figure(figsize = (10, 10))\nplt.imshow(wc)\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n2.5.4 언론사별 상대 빈도 분석 –&gt; 파이낸셜\n\nfin_words = df[df['언론사'] == '파이낸셜뉴스']\nfin_words = fin_words['키워드']\n\ntfidf = TfidfVectorizer()\ntdm = tfidf.fit_transform(fin_words)\n\nword_count4 = pd.DataFrame({\n    '단어': tfidf.get_feature_names_out(),\n    '빈도': tdm.sum(axis=0).flat\n}).sort_values('빈도', ascending = False).reset_index(drop = True)\n\nwc = WordCloud(font_path = 'malgun',\n    width = 500,\n    height = 500,\n    background_color='white').generate_from_frequencies(word_count4.set_index('단어').to_dict()[\"빈도\"])\n\nplt.figure(figsize = (10, 10))\nplt.imshow(wc)\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n2.5.5 언론사별 상대 빈도 분석 –&gt; 헤럴드\n\nhero_words = df[df['언론사'] == '헤럴드경제']\nhero_words = hero_words['키워드']\n\ntfidf = TfidfVectorizer()\ntdm = tfidf.fit_transform(hero_words)\n\nword_count5 = pd.DataFrame({\n    '단어': tfidf.get_feature_names_out(),\n    '빈도': tdm.sum(axis=0).flat\n}).sort_values('빈도', ascending = False).reset_index(drop = True)\n\nwc = WordCloud(font_path = 'malgun',\n    width = 500,\n    height = 500,\n    background_color='white').generate_from_frequencies(word_count5.set_index('단어').to_dict()[\"빈도\"])\n\nplt.figure(figsize = (10, 10))\nplt.imshow(wc)\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n2.5.6 상대 빈도 별 순위 시각화\n각 언론사 별 상대 빈도 순위를 비교한 값은 다음과 같다.\n\nwords_df1 = pd.concat([word_count1, word_count2], join='outer', axis=1)\nwords_df1.columns = ['단어(한경)', '빈도(한경)', '단어(매경)', '빈도(매경)']\nwords_df2 = pd.concat([word_count3, word_count4, word_count5], join='outer', axis=1)\nwords_df2.columns = ['단어(서경)', '빈도(서경)','단어(파이낸셜)', '빈도(파이낸셜)','단어(헤럴드)', '빈도(헤럴드)']\n\nwords_df = pd.concat([words_df1, words_df2], join = 'outer', axis = 1)\n\nwords_df.head(20)\n\n\n\n\n\n\n\n\n단어(한경)\n빈도(한경)\n단어(매경)\n빈도(매경)\n단어(서경)\n빈도(서경)\n단어(파이낸셜)\n빈도(파이낸셜)\n단어(헤럴드)\n빈도(헤럴드)\n\n\n\n\n0\n정부\n170.874039\n정부\n127.335164\n정부\n207.973736\n정부\n96.774121\n정부\n68.249231\n\n\n1\n기업\n143.353837\n기업\n110.468213\n북한\n153.504708\n대통령\n78.668422\n대통령\n55.984063\n\n\n2\n경제\n109.593958\n대통령\n105.349634\n기업\n147.576313\n기업\n73.745679\n경제\n47.226472\n\n\n3\n규제\n99.747167\n경제\n98.987800\n경제\n141.483724\n경제\n66.228374\n국민\n43.421289\n\n\n4\n한국\n96.602629\n북한\n94.959321\n대통령\n136.803875\n미국\n60.756488\n기업\n43.096729\n\n\n5\n정책\n95.039328\n미국\n93.682823\n정책\n125.951686\n일자리\n58.554333\n정책\n41.426563\n\n\n6\n미국\n90.302124\n중국\n87.481722\n미국\n125.941853\n한국\n57.845017\n검찰\n36.136012\n\n\n7\n대통령\n89.964928\n한국\n86.394644\n일자리\n108.601917\n정책\n53.730281\n상황\n35.747487\n\n\n8\n북한\n86.210303\n정책\n76.064262\n중국\n104.657793\n시장\n51.787001\n미국\n34.320186\n\n\n9\n중국\n83.305281\n국민\n73.182543\n규제\n100.005150\n규제\n45.949896\n국회\n33.419709\n\n\n10\n국민\n79.407091\n규제\n68.673798\n국민\n97.847511\n국회\n43.571681\n북한\n32.206295\n\n\n11\n일자리\n79.164376\n국회\n64.407326\n임금\n88.199132\n중국\n42.673492\n장관\n32.127931\n\n\n12\n시장\n68.216227\n시장\n63.713174\n한국\n84.390802\n일본\n40.091627\n한국\n31.377541\n\n\n13\n노조\n67.212984\n투자\n61.656313\n국회\n83.832130\n성장\n39.962075\n일자리\n30.921592\n\n\n14\n여당\n64.899902\n일자리\n60.358299\n투자\n81.507045\n임금\n39.627805\n성장\n30.897654\n\n\n15\n투자\n64.160446\n임금\n59.657628\n시장\n80.210144\n북한\n39.523417\n임금\n30.480363\n\n\n16\n임금\n62.896762\n일본\n58.333997\n성장\n79.884542\n국민\n38.434493\n대표\n29.536249\n\n\n17\n산업\n60.127337\n성장\n52.338215\n장관\n74.306424\n부동산\n35.483854\n인상\n28.477505\n\n\n18\n일본\n58.510270\n산업\n52.099878\n산업\n73.267545\n후보\n34.080738\n정도\n28.369887\n\n\n19\n국회\n58.063480\n장관\n51.585880\n인상\n73.013147\n반도체\n33.821207\n정치\n27.913461"
  },
  {
    "objectID": "economic_columns_clustering.html#dimension-reduction",
    "href": "economic_columns_clustering.html#dimension-reduction",
    "title": "2  Clustering analysis을 중심으로",
    "section": "2.6 Dimension Reduction",
    "text": "2.6 Dimension Reduction\n본격적인 군집 분석에 앞서, 데이터의 분포를 확인하고자 차원 축소를 진행하였다.\n차원 축소 진행을 위해 먼저 데이터 벡터화를 진행하였다.\n다만, 모든 데이터를 전부 넣을 경우, 컴퓨터가 연산량을 이기지 못하고 다운되는 경우가 재현되어, 데이터는 2020년 이후의 데이터로만 분석을 진행하였다.\n전체 데이터에 대한 분석은 2편의 토픽 모델링에서 진행하였다.\n\ndef targeting(x):\n    if x == '한국경제':\n        return 0\n    elif x == '매일경제':\n        return 1\n    elif x == '서울경제':\n        return 2\n    elif x == '파이낸셜뉴스':\n        return 3\n    elif x == '헤럴드경제':\n        return 4\n    elif x == '아시아경제':\n        return 5\n    \ndf['target'] = df['언론사'].apply(lambda x : targeting(x))\n\ndf20 = df[df['일자'] &gt;= 20200100]\ndf10 = df[df['일자'] &lt; 20200100]\n\ntext20 = df20['키워드']\ntext20_df = df20[['언론사', '제목']]\n\n가장 먼저 차원 축소 방법론인 PCA(Principle Component Analysis)를 사용하였다. 해당 방법을 통해 데이터를 2차원으로 축소하였다.\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n])        \nvec = pipeline.fit_transform(text20).toarray()\n\npca_df = PCA(n_components=2).fit_transform(vec)\n\npca_df = pd.DataFrame(pca_df, columns = ['component 0', 'component 1'])\n\n시각화 결과는 다음과 같다.\n\npca_df['target'] = df['target']\n\npca_0 = pca_df[pca_df['target'] == 0]\npca_1 = pca_df[pca_df['target'] == 1]\npca_2 = pca_df[pca_df['target'] == 2]\npca_3 = pca_df[pca_df['target'] == 3]\npca_4 = pca_df[pca_df['target'] == 4]\npca_5 = pca_df[pca_df['target'] == 5]\n\nplt.scatter(pca_0['component 0'], pca_0['component 1'], color = 'blue', label = '한국경제')\nplt.scatter(pca_1['component 0'], pca_1['component 1'], color = 'orange', label = '매일경제')\nplt.scatter(pca_2['component 0'], pca_2['component 1'], color = 'green', label = '서울경제')\nplt.scatter(pca_3['component 0'], pca_3['component 1'], color = 'yellow', label = '파이낸셜')\nplt.scatter(pca_4['component 0'], pca_4['component 1'], color = 'pink', label = '헤럴드')\nplt.scatter(pca_5['component 0'], pca_5['component 1'], color = 'purple', label = '아시아경제')\n\nplt.xlabel('component 0')\nplt.ylabel('component 1')\nplt.legend()\nplt.show()\n\n\n\n\n추가로 흝어진 정도를 파악하기 위해 T-SNE 방법을 사용했다.\n\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, learning_rate=400).fit_transform(vec)\n\ntsne_df = pd.DataFrame(tsne, columns = ['component 0', 'component 1'])\n\n시각화 결과는 다음과 같다.\n\ntsne_df['target'] = df['target']\n\ntsne_0 = tsne_df[tsne_df['target'] == 0]\ntsne_1 = tsne_df[tsne_df['target'] == 1]\ntsne_2 = tsne_df[tsne_df['target'] == 2]\ntsne_3 = tsne_df[tsne_df['target'] == 3]\ntsne_4 = tsne_df[tsne_df['target'] == 4]\ntsne_5 = tsne_df[tsne_df['target'] == 5]\n\n\nplt.scatter(tsne_0['component 0'], tsne_0['component 1'], color = 'blue', label = '한국경제')\nplt.scatter(tsne_1['component 0'], tsne_1['component 1'], color = 'orange', label = '매일경제')\nplt.scatter(tsne_2['component 0'], tsne_2['component 1'], color = 'green', label = '서울경제')\nplt.scatter(tsne_3['component 0'], tsne_3['component 1'], color = 'yellow', label = '파이낸셜')\nplt.scatter(tsne_4['component 0'], tsne_4['component 1'], color = 'pink', label = '헤럴드')\nplt.scatter(tsne_5['component 0'], tsne_5['component 1'], color = 'purple', label = '아시아경제')\n\nplt.xlabel('component 0')\nplt.ylabel('component 1')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "economic_columns_clustering.html#k-means-clustering",
    "href": "economic_columns_clustering.html#k-means-clustering",
    "title": "2  Clustering analysis을 중심으로",
    "section": "2.7 K-Means Clustering",
    "text": "2.7 K-Means Clustering\n본격적으로 군집 분석에 들어가기 앞서, K-Means는 최적 군집 갯수를 정해줘야 한다.\n최적 군집 갯수 추론을 위해 Elbow Method를 사용하였다.\n\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\n\nvzr = KElbowVisualizer(KMeans(), k=(2, 20))\nvzr.fit(pca_df)\nvzr.poof()\n\n\n\n\n&lt;Axes: title={'center': 'Distortion Score Elbow for KMeans Clustering'}, xlabel='k', ylabel='distortion score'&gt;\n\n\n분석 결과, 최적 토픽 갯수는 5개로 나타났다.\n해당 5개의 군집 분석 결과를 확인하기 위해 Silhouette Score 분석을 진행하였다. 분석 최적화를 위해 정규화도 진행하였다.\n\nfrom sklearn.preprocessing import Normalizer\n\nnor = Normalizer()\nvec_nor = nor.fit_transform(vec)\n\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nkmeans= KMeans(n_clusters=5, max_iter=1000, random_state=0) #최적 Topic 개수 5개를 기점으로 진행\nvisualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')\n\n\nvisualizer.fit(vec_nor)\nvisualizer.show()\n\n\n\n\n&lt;Axes: title={'center': 'Silhouette Plot of KMeans Clustering for 8433 Samples in 5 Centers'}, xlabel='silhouette coefficient values', ylabel='cluster label'&gt;\n\n\n생각보다 실루엣 계수가 많이 낮게 나타났다.\n이러한 결과가 K-means 분석 모델의 결과가 안좋은 것인지, 텍스트 데이터 분석 방법론으로서 k-Means의 한계인 것인치는 확인이 필요해보인다.\nK-Means 군집 별 기사 갯수는 다음과 같다.\n\nkmeans.fit(vec_nor)\n\nlabels = kmeans.labels_\n\ntext20_df['군집'] = labels\n\ntext20_df.groupby('군집').size()\n\n군집\n0     514\n1    1784\n2    4638\n3     807\n4     690\ndtype: int64\n\n\n또한 이후 개별적으로 분석해낸 토픽의 양상은 다음과 같다.\n0번 토픽 = 검찰 관련 칼럼\n1번 토픽 = 총선 관련 칼럼\n2번 토픽 = 경제 정책 관련 칼럼\n3번 토픽 = 부동산 관련 칼럼\n4번 토픽 = 북한 관련 칼럼\n해당 결과를 토대로 PCA와 T-SNE를 진행한 결과는 다음과 같다.\n\n2.7.1 PCA with K-means\n\npca_df['cluster'] = labels\n\npca_clu_0 = pca_df[pca_df['cluster'] == 0]\npca_clu_1 = pca_df[pca_df['cluster'] == 1]\npca_clu_2 = pca_df[pca_df['cluster'] == 2]\npca_clu_3 = pca_df[pca_df['cluster'] == 3]\npca_clu_4 = pca_df[pca_df['cluster'] == 4]\n\nplt.scatter(pca_clu_0['component 0'], pca_clu_0['component 1'], color = 'blue', label = '부동산')\nplt.scatter(pca_clu_1['component 0'], pca_clu_1['component 1'], color = 'orange', label = '코로나')\nplt.scatter(pca_clu_2['component 0'], pca_clu_2['component 1'], color = 'green', label = '북한')\nplt.scatter(pca_clu_3['component 0'], pca_clu_3['component 1'], color = 'purple', label = '경제 정책')\nplt.scatter(pca_clu_4['component 0'], pca_clu_4['component 1'], color = 'red', label = '정치')\n\nplt.xlabel('component 0')\nplt.ylabel('component 1')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n2.7.2 T-SNE with K-means\n\ntsne_df['cluster'] = labels\n\ntsne_clu0 = tsne_df[tsne_df['cluster'] == 0]\ntsne_clu1 = tsne_df[tsne_df['cluster'] == 1]\ntsne_clu2 = tsne_df[tsne_df['cluster'] == 2]\ntsne_clu3 = tsne_df[tsne_df['cluster'] == 3]\ntsne_clu4 = tsne_df[tsne_df['cluster'] == 4]\n\n# target 별 시각화\nplt.scatter(tsne_clu0['component 0'], tsne_clu0['component 1'], color = 'blue', label = '부동산')\nplt.scatter(tsne_clu1['component 0'], tsne_clu1['component 1'], color = 'orange', label = '코로나')\nplt.scatter(tsne_clu2['component 0'], tsne_clu2['component 1'], color = 'green', label = '북한')\nplt.scatter(tsne_clu3['component 0'], tsne_clu3['component 1'], color = 'purple', label = '경제 정책')\nplt.scatter(tsne_clu4['component 0'], tsne_clu4['component 1'], color = 'red', label = '정치')\n\nplt.xlabel('component 0')\nplt.ylabel('component 1')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "economic_topic_modeling.html#개요",
    "href": "economic_topic_modeling.html#개요",
    "title": "3  Topic Modeling을 중심으로",
    "section": "3.1 개요",
    "text": "3.1 개요\n본 내용은 1편에 이어 단어 군집 분석 방법인 토픽 모델링으로 분석을 진행한 보고서이다.\n분석 언어는 Python을 사용했으며, 가장 보편적인 토픽모델링 방법인 LDA(Latent Dirichlet Allocation)를 사용했다.\n알고리즘은 gensim 라이브러리의 LDA 모델을 사용했으며, 중간에 하이퍼파라미터 튜닝의 작업을 거쳤다."
  },
  {
    "objectID": "economic_topic_modeling.html#최적-토픽-갯수-추론",
    "href": "economic_topic_modeling.html#최적-토픽-갯수-추론",
    "title": "3  Topic Modeling을 중심으로",
    "section": "3.2 최적 토픽 갯수 추론",
    "text": "3.2 최적 토픽 갯수 추론\n먼저 해당 기사의 최적 토픽 갯수를 추론하기 위해 전처리의 과정을 거쳤다.\n\nkeywords = bkp.keywords_list(df)\nnews_words = bkp.keyword_parser(keywords)\nnews_dict = gensim.corpora.Dictionary(news_words)\ncorpus = [news_dict.doc2bow(text) for text in news_words]\n\n\n3.2.1 Perplexity\n가장 먼저 Perplexity를 통해 최적 토픽 갯수 추론을 시도하였다. Perplexity란, 선정된 토픽 개수마다 학습시켜 가장 낮은 값을 보이는 구간을 찾아 최적화된 토픽의 개수 추론하는 방법론이다.\n이는 확률 모델이 결과를 얼마나 정확하게 예측하는지 판단하는 척도로도 활용된다.\n\n\n\n\nperplexity\n\n\n그러나 결과적으로 Perplexity는 음수를 나타내고 있어, 해당 문서의 최적 토픽 갯수 추론으로 적합한 방법이 아닌 것으로 밝혀졌다.\n\n\n3.2.2 Coherence Score\n그 다음으로 Coherence Score를 통해 최적 토픽 갯수 추론을 시도하였다. Coherence는 반대로 선정된 토픽 개수마다 학습시켜 가장 높은 값을 보이는 구간을 찾아 최적화된 토픽의 개수 추론하는 방법론이다.\n이는 토픽이 얼마나 의미론적으로 일관성 있는지 판단하는 척도로 활용된다.\n\n\n\n\nCoherence\n\n\n분석 결과, 18개에서 가장 높은 Coherence score가 나타났다."
  },
  {
    "objectID": "economic_topic_modeling.html#토픽-모델링",
    "href": "economic_topic_modeling.html#토픽-모델링",
    "title": "3  Topic Modeling을 중심으로",
    "section": "3.3 토픽 모델링",
    "text": "3.3 토픽 모델링\nCoherence score를 토대로, 모델 학습을 진행하였다. 정확도를 높이기 위해 pass는 15, 반복 횟수는 100회로 높게 잡았다.\n토픽 별 상위 5개 단어는 하단과 같다.\n\nNUM_TOPICS = 18\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=news_dict, passes=15, iterations=100)\ntopics = ldamodel.print_topics(num_words=5)\nfor topic in topics:\n    print(topic)\n\n(0, '0.021*\"공공\" + 0.020*\"국민\" + 0.019*\"연금\" + 0.019*\"정부\" + 0.018*\"의료\"')\n(1, '0.033*\"금융\" + 0.024*\"규제\" + 0.024*\"은행\" + 0.020*\"시장\" + 0.015*\"서비스\"')\n(2, '0.033*\"정부\" + 0.022*\"예산\" + 0.020*\"재정\" + 0.015*\"지원\" + 0.011*\"국가\"')\n(3, '0.030*\"기업\" + 0.014*\"경영\" + 0.011*\"투자\" + 0.010*\"정부\" + 0.008*\"구조\"')\n(4, '0.049*\"북한\" + 0.034*\"미국\" + 0.023*\"대통령\" + 0.017*\"트럼프\" + 0.012*\"회담\"')\n(5, '0.024*\"국민\" + 0.023*\"후보\" + 0.023*\"선거\" + 0.022*\"정치\" + 0.019*\"대표\"')\n(6, '0.049*\"국회\" + 0.018*\"여당\" + 0.015*\"여야\" + 0.015*\"법안\" + 0.013*\"의원\"')\n(7, '0.035*\"일자리\" + 0.032*\"임금\" + 0.023*\"정부\" + 0.022*\"최저\" + 0.018*\"고용\"')\n(8, '0.036*\"기업\" + 0.029*\"규제\" + 0.021*\"산업\" + 0.019*\"정부\" + 0.018*\"경제\"')\n(9, '0.041*\"중국\" + 0.023*\"미국\" + 0.021*\"한국\" + 0.014*\"일본\" + 0.013*\"반도체\"')\n(10, '0.018*\"사고\" + 0.017*\"발생\" + 0.017*\"안전\" + 0.013*\"처벌\" + 0.011*\"사회\"')\n(11, '0.042*\"대통령\" + 0.027*\"정부\" + 0.020*\"청와대\" + 0.019*\"장관\" + 0.016*\"정책\"')\n(12, '0.033*\"경제\" + 0.018*\"성장\" + 0.012*\"한국\" + 0.010*\"경기\" + 0.010*\"금리\"')\n(13, '0.037*\"원전\" + 0.026*\"정부\" + 0.018*\"탈원전\" + 0.017*\"에너지\" + 0.014*\"정책\"')\n(14, '0.055*\"교육\" + 0.047*\"대학\" + 0.026*\"교육부\" + 0.024*\"인재\" + 0.017*\"학교\"')\n(15, '0.031*\"검찰\" + 0.026*\"수사\" + 0.020*\"의혹\" + 0.015*\"사건\" + 0.009*\"비리\"')\n(16, '0.025*\"정부\" + 0.024*\"부동산\" + 0.018*\"시장\" + 0.018*\"주택\" + 0.017*\"서울\"')\n(17, '0.043*\"노조\" + 0.018*\"일본\" + 0.013*\"노사\" + 0.013*\"파업\" + 0.010*\"요구\"')\n\n\n해당 결과를 LDAvis를 통하여 시각화 작업을 진행하였다.\n\nimport pyLDAvis.gensim_models\n\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim_models.prepare(ldamodel, corpus, news_dict)\npyLDAvis.display(vis)"
  }
]